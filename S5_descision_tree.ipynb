{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea109c7",
   "metadata": {},
   "source": [
    "# Descision Tree Classifier\n",
    "\n",
    "In descision Trees we try to make yes/no descisions at each node of the tree and based on the threshold and feature we either move left or right. After reaching the end (leaf node) majority class in that node will be the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a330826",
   "metadata": {},
   "source": [
    "## Measures for splitting\n",
    "Each Node of the tree represents group of data, and it has some impurity value. Impurity is nothing but measure of mixedness/randomness in data.\n",
    "Commonly used measures of calculating impurity are Gini and Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394b4c0",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "$ I_{G} = 1-\\sum_{i=1}^{k} P(k)^{2} $\n",
    "\n",
    "For example,  \n",
    "$ class = [1,1,1], I_{G}= 0 $ (zero means pure)  \n",
    "$ class = [1,1,0,0], I_{G}= 0.5 $ (Max for binary classification)   \n",
    "$ class = [1,1,1,0], I_{G}= 0.375 $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6bf2b",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "$ I_{E} = -\\sum_{i=1}^{k} P(k)\\log_{2} {P(k)} $\n",
    "\n",
    "For example,  \n",
    "$ class = [1,1,1], I_{E}= 0 $ (zero means pure)  \n",
    "$ class = [1,1,0,0], I_{E}= 1 $ (Max impure or equal classes)   \n",
    "$ class = [1,1,1,0], I_{E}= 0.811 $  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8d4f2",
   "metadata": {},
   "source": [
    "### Weighted Impurity\n",
    "Weighted impurity is the total proportionally summed impurity of children of a Node.  \n",
    "Suppose for root node, two splits occur(that is two child nodes are formed), and the impurity of each child node is I1 and I2, then \n",
    "   \n",
    "$ WI = \\dfrac{sample\\_count_{1}}{parent\\_sample\\_count} * I1 + \\dfrac{sample\\_count_{2}}{parent\\_sample\\_count} * I2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be7b17",
   "metadata": {},
   "source": [
    "### Gain (Information Gain)\n",
    "\n",
    "Information Gain shows how much better the split has happend for a parent for that particular split.\n",
    "\n",
    "IG(of a split) = Parent_entropy - Weighted_impurity(of that split)  \n",
    "  \n",
    "Gain is used to decide the feature and threshold of each node, the max the gain the better the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3699b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Node:\n",
    "    def __init__(self,* , left=None, right=None, feature=None, threshold=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.right = right\n",
    "        self.left = left\n",
    "        self.value = value\n",
    "    \n",
    "    def _is_leaf(self):\n",
    "        return self.value is not None\n",
    "    \n",
    "class DescisionTreeClassifier:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, criterion = \"gini\"):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "    \n",
    "    def _gini(self,y):\n",
    "        _ , count = np.unique(y, return_counts=True)\n",
    "        total = np.sum(count)\n",
    "        probsquare_array = [(c/total)**2 for c in count]\n",
    "        return 1-np.sum(probsquare_array)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        _ , count = np.unique(y, return_counts=True)\n",
    "        total = np.sum(count)\n",
    "        prob_array = [(c/total) for c in count]\n",
    "        return -1*np.sum([p*np.log2(p) for p in prob_array])\n",
    "    \n",
    "    def impurity(self, y):\n",
    "        if self.criterion==\"gini\":\n",
    "            return self._gini(y)\n",
    "        else:\n",
    "            return self._entropy(y)\n",
    "        \n",
    "    def _best_split(self, X,Y):\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples <= 1:\n",
    "            return None, None, None, None, None\n",
    "        \n",
    "        parent_imp = self.impurity(Y)\n",
    "        gain = 0\n",
    "        threshold = 0\n",
    "        feature = None\n",
    "        right = None\n",
    "        left = None\n",
    "\n",
    "        for current_feature in range(n_features):\n",
    "            sorted_fea = np.argsort(X[:,current_feature]) #So that while sorting both X and y ,order matches\n",
    "            X_sorted, Y_sorted = X[sorted_fea, current_feature], Y[sorted_fea]\n",
    "            for i in range(1,n_samples):\n",
    "                if X_sorted[i]==X_sorted[i-1]:\n",
    "                    continue\n",
    "                current_threshold = (X_sorted[i]+X_sorted[i-1])/2\n",
    "                current_left_idx = sorted_fea[:i]\n",
    "                current_right_idx = sorted_fea[i:]\n",
    "\n",
    "                if len(current_left_idx)==0 or len(current_right_idx)==0:\n",
    "                    continue\n",
    "                left_imp = self.impurity(Y[current_left_idx])\n",
    "                right_imp = self.impurity(Y[current_right_idx])\n",
    "                weighted_imp = (len(current_left_idx)*left_imp+len(current_right_idx)*right_imp)/n_samples\n",
    "                current_gain = parent_imp-weighted_imp\n",
    "                if current_gain>gain:\n",
    "                    gain = current_gain\n",
    "                    threshold = current_threshold\n",
    "                    feature = current_feature\n",
    "                    right = current_right_idx\n",
    "                    left = current_left_idx\n",
    "\n",
    "        return gain, threshold, feature, left, right\n",
    "    \n",
    "    def _build_tree(self, X, Y, depth = 0):\n",
    "        values, count = np.unique(Y, return_counts=True)\n",
    "        if depth>=self.max_depth or len(values)==1 or len(Y)<self.min_samples_split:\n",
    "            leaf_value = values[np.argmax(count)]\n",
    "            return Node(value=leaf_value)\n",
    "        gain, th_hold, fea, leftid, rightid = self._best_split(X,Y)\n",
    "        if gain==0 or leftid is None or rightid is None:\n",
    "            leaf_value = values[np.argmax(count)]\n",
    "            return Node(value=leaf_value)\n",
    "        left = self._build_tree(X[leftid], Y[leftid], depth+1)\n",
    "        right = self._build_tree(X[rightid], Y[rightid], depth+1)\n",
    "        return Node(left=left, right=right, feature=fea, threshold=th_hold)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.root = self._build_tree(X,Y)\n",
    "\n",
    "    def _predict_one(self, X, node: Node):\n",
    "        if node._is_leaf():\n",
    "            return node.value\n",
    "        if X[node.feature]<=node.threshold:\n",
    "            return self._predict_one(X, node.left)\n",
    "        else:\n",
    "            return self._predict_one(X, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0360ffd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:/Ml/part1/data/diabetes.csv\")\n",
    "X = df.drop(columns=[\"Outcome\"]).values\n",
    "Y = df['Outcome'].values\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b72f94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "593d9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DescisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14412258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating predicted values\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "929ef025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7597402597402597"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c058aee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83, 16],\n",
       "       [21, 34]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
