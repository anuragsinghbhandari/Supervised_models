{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b620fa0d",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "Descision Trees are high in variance, a small change in data can lead to overall change in structure of the tree.  \n",
    "So to tackle this problems various ensemble Methods are used.  \n",
    "  \n",
    "We will discuss three commonly used ensemble Methods:-  \n",
    "1. Bagging (Bootstrap Aggregator)\n",
    "2. Boosting\n",
    "3. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db188dba",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "In bagging we try to train multiple descision trees, each tree will be trained on unique dataset, though the dataset is the same as original dataset but some values will be replaced by duplicate values, hence creating a different view for each tree or we can say each tree will be biased.  \n",
    "Then during prediction each tree will predict, and the major vote will be chosen(in case of classification) or average will be taken (in case of regression).  \n",
    "Random Forest is an example of Bagging.\n",
    "\n",
    "<i><u>Bagging is not entitled to Decision Trees, Descision trees are high in variance so Bagging helps a lot, so we used bagging with Descision Trees here.</i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef9ca275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BaggingClassifier:\n",
    "    def __init__(self,base_classifier_function, n_estimator=10):\n",
    "        self.base_classifier = base_classifier_function\n",
    "        self.n_estimator = n_estimator\n",
    "        self.models = []\n",
    "    \n",
    "    def _bagging_samples(self, X, Y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        return X[indices], Y[indices]\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.models = []\n",
    "        for _ in range(self.n_estimator):\n",
    "            model = self.base_classifier()\n",
    "            new_X, new_Y = self._bagging_samples(X,Y)\n",
    "            model.fit(new_X, new_Y)\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.array([model.predict(X) for model in self.models])\n",
    "        final_preds = []\n",
    "        for each_row in preds.T:\n",
    "            values, count = np.unique(each_row, return_counts=True)\n",
    "            final_preds.append(values[np.argmax(count)])\n",
    "        return np.array(final_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a07ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:/Ml/part1/data/diabetes.csv\")\n",
    "X = df.drop(columns=[\"Outcome\"]).values\n",
    "Y = df['Outcome'].values\n",
    "type(X)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22553f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def new_tree():\n",
    "    return DecisionTreeClassifier()\n",
    "model = BaggingClassifier(new_tree)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8c1b145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating predicted values\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d3f820b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857142857142857"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36527929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87, 12],\n",
       "       [21, 34]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a9465",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "In Boosting the idea is to train weak learners( weak learners are one depth decision tree), these weak learners then together predict the output (unlike bagging where each indepently predicts), the impact of each weak learner is decided by its weights, more the weight greater the contribution.  \n",
    "Weak learners are called decision stumps because it’s a “tiny tree with just a stump” (no branches beyond the first split)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b37d7",
   "metadata": {},
   "source": [
    "Each weak learner tries to learn from previous learner, focusing more on what it was missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da1c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Desicionstump:\n",
    "    def __init__(self):\n",
    "        self.min_error = float('inf')\n",
    "        self.p = None\n",
    "        self.f_id = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def fit(self,X,Y, weights):\n",
    "        rows, columns = X.shape\n",
    "        for feature_id in range(columns):\n",
    "            sort_ids = np.argsort(X[:,feature_id])\n",
    "            sortedX = X[sort_ids, feature_id]\n",
    "            sortedY = Y[sort_ids]\n",
    "            for i in range(1,rows):\n",
    "                for polarity in [1,-1]:\n",
    "                    current_threshold = (sortedX[i]+sortedX[i-1])/2\n",
    "                    if polarity == 1:\n",
    "                        pred = [1 if each<current_threshold else -1 for each in sortedX ]\n",
    "                    else:\n",
    "                        pred = [-1 if each<current_threshold else 1 for each in sortedX ]\n",
    "                    weighted_error = np.sum(weights*(pred!=sortedY))\n",
    "                    if weighted_error<self.min_error:\n",
    "                        self.min_error = weighted_error\n",
    "                        self.f_id = feature_id\n",
    "                        self.threshold = current_threshold\n",
    "                        self.p = polarity\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        Y_pred = self.p * np.array([1 if each<self.threshold else -1 for each in X[:,self.f_id]])\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95154121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(self, n_estimators: int = 100):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learners = []\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        Y = np.where(Y<=0, -1, 1)\n",
    "        rows, columns = X.shape\n",
    "        self.learners = []\n",
    "        weights = np.ones(rows)/rows\n",
    "        for _ in range(self.n_estimators):\n",
    "            stump = Desicionstump()\n",
    "            stump.fit(X,Y,weights=weights)\n",
    "            stump.alpha = 0.5*(np.log((1-stump.min_error)/(stump.min_error + 1e-10)))\n",
    "            final_pred = stump.predict(X)\n",
    "            weights = weights*np.exp(-1*stump.alpha*final_pred*Y)\n",
    "            weights /= np.sum(weights) # Normalized weights\n",
    "            self.learners.append(stump)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for learners in self.learners:\n",
    "            stump_pred = learners.alpha*learners.predict(X)\n",
    "            predictions.append(stump_pred)\n",
    "        final_pred = np.sign(np.sum(predictions, axis=0))\n",
    "        final_pred = np.where(final_pred<=0, 0, 1)\n",
    "        return final_pred\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2baf0f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:/Ml/part1/data/diabetes.csv\")\n",
    "X = df.drop(columns=[\"Outcome\"]).values\n",
    "Y = df['Outcome'].values\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b597defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ff26ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Adaboost(n_estimators=10)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50ad723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating predicted values\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a84726c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7402597402597403"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19864dcf",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "Idea is simple multiple different models predicting and a boss model is watching their predictions and learning from their prediction and actual answer.\n",
    "\n",
    "We use various folds to train and test each model so that models predict on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d90f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class StackingClassifier:\n",
    "    def __init__(self, base_models: list, meta_model, folds: int):\n",
    "        self.models = base_models\n",
    "        self.folds = folds\n",
    "        self.meta_model = meta_model\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        n_samples = X.shape[0]\n",
    "        batch_size = n_samples//self.folds\n",
    "        X_final = np.zeros((n_samples, len(self.models)))\n",
    "        for i,model in enumerate(self.models):\n",
    "            cummaltive_pred = np.array([])\n",
    "            for fold in range(self.folds):\n",
    "                start = fold*batch_size\n",
    "                end = (fold+1)*batch_size\n",
    "                if fold == self.folds-1:\n",
    "                    X_test = X[start:]\n",
    "                else:\n",
    "                    X_test = X[start:end]\n",
    "                X_train = np.concatenate([X[:start],X[end:]], axis=0)\n",
    "                Y_train = np.concatenate([Y[:start],Y[end:]], axis=0)\n",
    "                model.fit(X_train, Y_train)\n",
    "                pred = model.predict(X_test)\n",
    "                cummaltive_pred = np.concatenate([cummaltive_pred, pred])\n",
    "\n",
    "            model.fit(X,Y)\n",
    "            X_final[:,i] = cummaltive_pred\n",
    "        self.meta_model.fit(X_final,Y)\n",
    "        \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_sample = X.shape[0]\n",
    "        X_final = np.zeros((n_sample,len(self.models)))\n",
    "        for i,model in enumerate(self.models):\n",
    "            pred = model.predict(X)\n",
    "            X_final[:,i] = pred\n",
    "        final_pred = self.meta_model.predict(X_final)\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a739189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:/Ml/part1/data/diabetes.csv\")\n",
    "X = df.drop(columns=[\"Outcome\"]).values\n",
    "Y = df['Outcome'].values\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a60d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116f159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models = [LogisticRegression(), SVC(), AdaBoostClassifier(), RandomForestClassifier()]\n",
    "model = StackingClassifier(base_models=models, meta_model=LogisticRegression(), folds=4)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "656f9e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating predicted values\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd278eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7467532467532467"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18c33e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[80, 19],\n",
       "       [20, 35]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
